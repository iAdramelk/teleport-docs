---
title: Dynamic Kubernetes Cluster Registration
description: Register and unregister Kubernetes clusters without restarting a Teleport Kubernetes Service instance.
version: '14.x'
---

With dynamic Kubernetes cluster registration, you can manage the Kubernetes
clusters connected to your Teleport cluster without needing to modify the
configuration file of an individual Kubernetes Service instance.

Dynamic Kubernetes cluster registration is useful when you have deployed
multiple Kubernetes Service instances or need to regularly reconfigure access to
Kubernetes clusters in your infrastructure.

In this guide, we will show you how to set up dynamic Kubernetes cluster
registration, then create, list, update, and delete Kubernetes clusters via
`tctl`.

## Prerequisites

<Tabs>
  <Tab title="Teleport Team">
    - A Teleport Team account. If you do not have one, visit the [signup
      page](https://goteleport.com/signup/) to begin your free trial.

    - The `tctl` admin tool and `tsh` client tool version >= 14.0.0-dev.

      ```code
      $ tctl version
      # Teleport v14.0.0-dev go1.20

      $ tsh version
      # Teleport v14.0.0-dev go1.20
      ```

      See [Installation](/ver/14.x/installation) for details.
  </Tab>

  <Tab title="Teleport Community Edition">
    - A running Teleport cluster. For details on how to set this up, see our
      [Getting Started](/ver/14.x/index) guide.

    - The `tctl` admin tool and `tsh` client tool version >= 14.0.0-dev.

      ```code
      $ tctl version
      # Teleport v14.0.0-dev go1.20

      $ tsh version
      # Teleport v14.0.0-dev go1.20
      ```

      See [Installation](/ver/14.x/installation) for details.
  </Tab>

  <Tab title="Teleport Enterprise">
    - A running Teleport Enterprise cluster. For details on how to set this up, see our Enterprise
      [Getting Started](/ver/14.x/choose-an-edition/teleport-enterprise/introduction) guide.

    - The Enterprise `tctl` admin tool and `tsh` client tool version >= 14.0.0-dev,
      which you can download by visiting your [Teleport account](https://teleport.sh).

      ```code
      $ tctl version
      # Teleport Enterprise v14.0.0-dev go1.20

      $ tsh version
      # Teleport v14.0.0-dev go1.20
      ```
  </Tab>

  <Tab title="Teleport Enterprise Cloud">
    - A Teleport Enterprise Cloud account. If you do not have one, visit the [signup
      page](https://goteleport.com/signup/) to begin a free trial of Teleport Team
      and upgrade to Teleport Enterprise Cloud.

    - The Enterprise `tctl` admin tool and `tsh` client tool version >= 13.2.0.
      To download these tools, visit the [Downloads](/ver/14.x/choose-an-edition/teleport-cloud/downloads) page.

      ```code
      $ tctl version
      # Teleport Enterprise v13.2.0 go1.20

      $ tsh version
      # Teleport v13.2.0 go1.20
      ```
  </Tab>
</Tabs>

- A Linux host where you will install the Teleport Kubernetes Service.

  <Tip>
    Our `teleport-kube-agent` Helm chart does not support dynamic Kubernetes
    cluster registration.
  </Tip>

- A Kubernetes cluster to join to your Teleport cluster. You must have
  permissions to create namespaces, secrets, service accounts, cluster roles,
  and cluster role bindings in the cluster.

- Make sure you can connect to Teleport. Log in to your cluster using `tsh`, then use `tctl`
  remotely:
  {/* Ignoring scope linting since we use this partial throughout the docs and
  cannot guarantee that it will line up with a page's configured scopes*/}
  {/*lint ignore scopes*/}
  ```code
  $ tsh login --proxy=teleport.example.com --user=email@example.com
  $ tctl status
  # Cluster  teleport.example.com
  # Version  14.0.0-dev
  # CA pin   sha256:abdc1245efgh5678abdc1245efgh5678abdc1245efgh5678abdc1245efgh5678
  ```
  You can run subsequent `tctl` commands in this guide on your local machine.

  For full privileges, you can also run `tctl` commands on your Auth Service host.
  {/*lint ignore scopes*/}
  ```code
  $ tsh login --proxy=myinstance.teleport.sh --user=email@example.com
  $ tctl status
  # Cluster  myinstance.teleport.sh
  # Version  13.2.0
  # CA pin   sha256:sha-hash-here
  ```
  You must run subsequent `tctl` commands in this guide on your local machine.

## Step 1/3. Set up the Teleport Kubernetes Service

The Teleport Kubernetes Service proxies traffic from Teleport users to a
Kubernetes API server so you can take advantage of passwordless authentication,
role-based access controls, audit logging, and other Teleport features in order
to manage access to Kubernetes.

In this step, you will install the Teleport Kubernetes Service on a Linux host
and configure it to access any Kubernetes cluster you register with your
Teleport cluster.

### Get a join token

Establish trust between your Teleport cluster and your new Kubernetes Service
instance by creating a join token:

```code
$ tctl tokens add --type=kube --ttl=1h --format=text
abcd123-insecure-do-not-use-this
```

Copy the token and keep it somewhere safe so you can use it when running the
Teleport Kubernetes Service.

### Install the Teleport Kubernetes Service

Install the Teleport Kubernetes Service on your Linux host:

Use the appropriate commands for your environment to install your package:

<Tabs dropdownView dropdownCaption="Teleport Edition">
  <Tab title="Teleport Team">
    ```code
    $ curl https://goteleport.com/static/install.sh | bash -s 13.2.0
    ```

    <Accordion title="Is my Teleport instance compatible with Teleport Team?">
      Before installing a `teleport` binary with a version besides
      v13, read our compatibility rules to ensure that the
      binary is compatible with Teleport Cloud.

      When running multiple `teleport` binaries within a cluster, the following rules
      apply:

      - **Patch and minor** versions are always compatible, for example, any 8.0.1
        component will work with any 8.0.3 component and any 8.1.0 component will work
        with any 8.3.0 component.
      - Servers support clients that are 1 major version behind, but do not support
        clients that are on a newer major version. For example, an 8.x.x Proxy Service
        is compatible with 7.x.x resource services and 7.x.x `tsh`, but we don't
        guarantee that a 9.x.x resource service will work with an 8.x.x Proxy Service.
        This also means you must not attempt to upgrade from 6.x.x straight to 8.x.x.
        You must upgrade to 7.x.x first.
      - Proxy Services and resource services do not support Auth Services that are on
        an older major version, and will fail to connect to older Auth Services by
        default. This behavior can be overridden by passing `--skip-version-check`
        when starting Proxy Services and resource services.
    </Accordion>
  </Tab>

  <Tab title="Open Source">
    ```code
    $ curl https://goteleport.com/static/install.sh | bash -s 14.0.0-dev
    ```
  </Tab>

  <Tab title="Enterprise">
    <Tabs>
      <Tab title="Debian 8+/Ubuntu 16.04+ (apt)">
        ```code
        # Download Teleport's PGP public key
        $ sudo curl https://apt.releases.teleport.dev/gpg \
        -o /usr/share/keyrings/teleport-archive-keyring.asc
        # Source variables about OS version
        $ source /etc/os-release
        # Add the Teleport APT repository for v14. You'll need to update this
        # file for each major release of Teleport.
        $ echo "deb [signed-by=/usr/share/keyrings/teleport-archive-keyring.asc] \
        https://apt.releases.teleport.dev/${ID?} ${VERSION_CODENAME?} stable/v14" \
        | sudo tee /etc/apt/sources.list.d/teleport.list > /dev/null

        $ sudo apt-get update
        $ sudo apt-get install teleport-ent
        ```

        For FedRAMP/FIPS-compliant installations, install the `teleport-ent-fips` package instead:

        ```code
        $ sudo apt-get install teleport-ent-fips
        ```
      </Tab>

      <Tab title="Amazon Linux 2/RHEL 7 (yum)">
        ```code
        # Source variables about OS version
        $ source /etc/os-release
        # Add the Teleport YUM repository for v14. You'll need to update this
        # file for each major release of Teleport.
        # First, get the major version from $VERSION_ID so this fetches the correct
        # package version.
        $ VERSION_ID=$(echo $VERSION_ID | grep -Eo "^[0-9]+")
        $ sudo yum-config-manager --add-repo "$(rpm --eval "https://yum.releases.teleport.dev/$ID/$VERSION_ID/Teleport/%{_arch}/stable/v14/teleport.repo")"
        $ sudo yum install teleport-ent
        #
        # Tip: Add /usr/local/bin to path used by sudo (so 'sudo tctl users add' will work as per the docs)
        # echo "Defaults    secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin" > /etc/sudoers.d/secure_path
        ```

        For FedRAMP/FIPS-compliant installations, install the `teleport-ent-fips` package instead:

        ```code
        $ sudo yum install teleport-ent-fips
        ```
      </Tab>

      <Tab title="Amazon Linux 2023/RHEL 8+ (dnf)">
        ```code
        # Source variables about OS version
        $ source /etc/os-release
        # Add the Teleport YUM repository for v14. You'll need to update this
        # file for each major release of Teleport.
        # Use the dnf config manager plugin to add the teleport RPM repo
        $ sudo dnf config-manager --add-repo "$(rpm --eval "https://yum.releases.teleport.dev/$ID/$VERSION_ID/Teleport/%{_arch}/stable/v14/teleport.repo")"

        # Install teleport
        $ sudo dnf install teleport-ent

        # Tip: Add /usr/local/bin to path used by sudo (so 'sudo tctl users add' will work as per the docs)
        # echo "Defaults    secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin" > /etc/sudoers.d/secure_path
        ```

        For FedRAMP/FIPS-compliant installations, install the `teleport-ent-fips` package instead:

        ```code
        $ sudo dnf install teleport-ent-fips
        ```
      </Tab>

      <Tab title="Tarball">
        In the example commands below, update `$SYSTEM_ARCH` with the appropriate
        value (`amd64`, `arm64`, or `arm`). All example commands using this variable
        will update after one is filled out.

        ```code
        $ curl https://get.gravitational.com/teleport-ent-v14.0.0-dev-linux-$SYSTEM_ARCH-bin.tar.gz.sha256
        # <checksum> <filename>
        $ curl -O https://cdn.teleport.dev/teleport-ent-v14.0.0-dev-linux-$SYSTEM_ARCH-bin.tar.gz
        $ shasum -a 256 teleport-ent-v14.0.0-dev-linux-$SYSTEM_ARCH-bin.tar.gz
        # Verify that the checksums match
        $ tar -xvf teleport-ent-v14.0.0-dev-linux-$SYSTEM_ARCH-bin.tar.gz
        $ cd teleport-ent
        $ sudo ./install
        ```

        For FedRAMP/FIPS-compliant installations of Teleport Enterprise, package URLs
        will be slightly different:

        ```code
        $ curl https://get.gravitational.com/teleport-ent-v14.0.0-dev-linux-$SYSTEM_ARCH-fips-bin.tar.gz.sha256
        # <checksum> <filename>
        $ curl -O https://cdn.teleport.dev/teleport-ent-v14.0.0-dev-linux-$SYSTEM_ARCH-fips-bin.tar.gz
        $ shasum -a 256 teleport-ent-v14.0.0-dev-linux-$SYSTEM_ARCH-fips-bin.tar.gz
        # Verify that the checksums match
        $ tar -xvf teleport-ent-v14.0.0-dev-linux-$SYSTEM_ARCH-fips-bin.tar.gz
        $ cd teleport-ent
        $ sudo ./install
        ```
      </Tab>
    </Tabs>
  </Tab>

  <Tab title="Enterprise Cloud">
    <Tabs>
      <Tab title="Debian 8+/Ubuntu 16.04+ (apt)">
        Add the Teleport repository to your repository list:

        ```code
        # Download Teleport's PGP public key
        $ sudo curl https://apt.releases.teleport.dev/gpg \
        -o /usr/share/keyrings/teleport-archive-keyring.asc
        # Source variables about OS version
        $ source /etc/os-release
        # Add the Teleport apt repository for cloud.
        $ echo "deb [signed-by=/usr/share/keyrings/teleport-archive-keyring.asc] \
        https://apt.releases.teleport.dev/${ID?} ${VERSION_CODENAME?} stable/cloud" \
        | sudo tee /etc/apt/sources.list.d/teleport.list > /dev/null

        $ sudo apt-get update
        $ sudo apt-get install teleport-ent
        ```
      </Tab>

      <Tab title="Amazon Linux 2/RHEL 7 (yum)">
        ```code
        # Source variables about OS version
        $ source /etc/os-release
        # Add the Teleport yum repository for cloud.
        # First, get the major version from $VERSION_ID so this fetches the correct
        # package version.
        $ VERSION_ID=$(echo $VERSION_ID | grep -Eo "^[0-9]+")
        $ sudo yum-config-manager --add-repo "$(rpm --eval "https://yum.releases.teleport.dev/$ID/$VERSION_ID/Teleport/%{_arch}/stable/cloud/teleport.repo")"
        $ sudo yum install teleport-ent
        #
        # Tip: Add /usr/local/bin to path used by sudo (so 'sudo tctl users add' will work as per the docs)
        # echo "Defaults    secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin" > /etc/sudoers.d/secure_path
        ```
      </Tab>

      <Tab title="Amazon Linux 2023/RHEL 8+ (dnf)">
        ```code
        # Source variables about OS version
        $ source /etc/os-release
        # Add the Teleport yum repository for cloud.
        # Use the dnf config manager plugin to add the teleport RPM repo
        $ sudo dnf config-manager --add-repo "$(rpm --eval "https://yum.releases.teleport.dev/$ID/$VERSION_ID/Teleport/%{_arch}/stable/cloud/teleport.repo")"

        # Install teleport
        $ sudo dnf install teleport-ent

        # Tip: Add /usr/local/bin to path used by sudo (so 'sudo tctl users add' will work as per the docs)
        # echo "Defaults    secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin" > /etc/sudoers.d/secure_path
        ```
      </Tab>

      <Tab title="Tarball">
        In the example commands below, update `$SYSTEM_ARCH` with the appropriate
        value (`amd64`, `arm64`, or `arm`). All example commands using this variable
        will update after one is filled out.

        ```code
        $ curl https://get.gravitational.com/teleport-ent-v13.2.0-linux-$SYSTEM_ARCH-bin.tar.gz.sha256
        # <checksum> <filename>
        $ curl -O https://cdn.teleport.dev/teleport-ent-v13.2.0-linux-amd64-bin.tar.gz
        $ shasum -a 256 teleport-ent-v13.2.0-linux-amd64-bin.tar.gz
        # Verify that the checksums match
        $ tar -xvf teleport-ent-v13.2.0-linux-amd64-bin.tar.gz
        $ cd teleport-ent
        $ sudo ./install
        ```
      </Tab>
    </Tabs>

    <Accordion title="Is my Teleport instance compatible with Teleport Enterprise Cloud?">
      Before installing a `teleport` binary with a version besides v13,
      read our compatibility rules to ensure that the binary is compatible with
      Teleport Enterprise Cloud.

      When running multiple `teleport` binaries within a cluster, the following rules
      apply:

      - **Patch and minor** versions are always compatible, for example, any 8.0.1
        component will work with any 8.0.3 component and any 8.1.0 component will work
        with any 8.3.0 component.
      - Servers support clients that are 1 major version behind, but do not support
        clients that are on a newer major version. For example, an 8.x.x Proxy Service
        is compatible with 7.x.x resource services and 7.x.x `tsh`, but we don't
        guarantee that a 9.x.x resource service will work with an 8.x.x Proxy Service.
        This also means you must not attempt to upgrade from 6.x.x straight to 8.x.x.
        You must upgrade to 7.x.x first.
      - Proxy Services and resource services do not support Auth Services that are on
        an older major version, and will fail to connect to older Auth Services by
        default. This behavior can be overridden by passing `--skip-version-check`
        when starting Proxy Services and resource services.
    </Accordion>
  </Tab>
</Tabs>

### Configure the Teleport Kubernetes Service

On the host where you will run the Teleport Kubernetes Service, run the
following command to create a base configuration for your Teleport instance,
assigning `PROXY_SERVICE` to the host and port of your Teleport Proxy Service or
Teleport Cloud tenant and `TOKEN` to the join token we created earlier:

```code
# e.g., teleport.example.com:443
$ PROXY_SERVICE=proxy-addr
# e.g., abcd123-insecure-do-not-use-this;
$ TOKEN=join-token;
$ sudo teleport configure \
--proxy=${PROXY_SERVICE?} \
--roles=kube \
--token=${TOKEN?} \
-o file
```

Edit your configuration file at `/etc/teleport.yaml` to include the following:

```yaml
kubernetes_service:
  enabled: "yes"
  resources:
  - labels:
      "*": "*"
```

This configuration enables your Kubernetes Service instance to connect to any
Kubernetes clusters you register with your Teleport cluster. This is because the
`resources[0].labels` field includes the wildcard pattern (`"*": "*"`), which
allows this Kubernetes Service instance to connect to Kubernetes cluster
resources with any label key or value.

<Accordion title="Selectively watching Kubernetes clusters">
  You can configure a Kubernetes Service instance to watch for a subset of
  Kubernetes clusters by including specific label keys and values instead of
  wildcard characters:

  ```yaml
  resources:
  - labels:
      "env": "prod"
      "region": "us-east-2"
  - labels:
      "env": "test"
      "region": "us-west-1"
  ```

  For the Kubernetes Service to register a cluster, *any* of the items in
  `resources` must match the cluster's labels. For an item in `resources` to
  match, *all* of the `labels` entries within that item must match the cluster's
  labels.

  For example, a cluster with the labels `env:prod` and `region:us-west-1` would
  not match the configuration above, since it only matches the `env:prod` label in
  the first `resources` item and the `region:us-west-1` label in the second
  `resources` item.

  However, a cluster with `env:test` and `region:us-west-1` would match, since it
  matches both labels given in the second `resources` item.

  When you create dynamic Kubernetes cluster resources later in this guide, you
  can assign them labels to ensure that only specific Kubernetes Service instances
  will watch for them.
</Accordion>

### Run the Teleport Kubernetes Service

Configure the Teleport Kubernetes Service to start automatically when the host boots up by
creating a systemd service for it. The instructions depend on how you installed
the Teleport Kubernetes Service.

<Tabs>
  <Tab title="Package Manager">
    On the host where you will run the Teleport Kubernetes Service, enable and start Teleport:

    ```code
    $ sudo systemctl enable teleport
    $ sudo systemctl start teleport
    ```
  </Tab>

  <Tab title="TAR Archive">
    On the host where you will run the Teleport Kubernetes Service, create a systemd service
    configuration for Teleport, enable the Teleport service, and start Teleport:

    ```code
    $ sudo teleport install systemd -o /etc/systemd/system/teleport.service
    $ sudo systemctl enable teleport
    $ sudo systemctl start teleport
    ```
  </Tab>
</Tabs>

You can check the status of the Teleport Kubernetes Service with `systemctl status teleport`
and view its logs with `journalctl -fu teleport`.

## Step 2/3. Authorize your user

To enable dynamic Kubernetes cluster registration in Teleport, you will need to
authorize your user to access the Kubernetes clusters you want to register with
Teleport. We will configure this access in this step, both in Teleport and on
your Kubernetes cluster.

### Allow access to your Kubernetes cluster

Ensure that you are in the correct Kubernetes context for the cluster you would
like to enable access to.

Retrieve all available contexts:

```code
$ kubectl config get-contexts
```

Switch to your context, replacing `CONTEXT_NAME` with the name of your chosen
context:

```code
$ kubectl config use-context CONTEXT_NAME
Switched to context CONTEXT_NAME
```

### Kubernetes authentication

To authenticate to a Kubernetes cluster via Teleport, your Teleport roles must
allow access as at least one Kubernetes user or group. Ensure that you have a
Teleport role that grants access to the cluster you plan to interact with.

Run the following command to get the Kubernetes user for your current context:

```code
$ kubectl config view \
-o jsonpath="{.contexts[?(@.name==\"$(kubectl config current-context)\")].context.user}"
```

Create a file called `kube-access.yaml` with the following content, replacing
`USER` with the output of the command above.

```yaml
kind: role
metadata:
  name: kube-access
version: v6
spec:
  allow:
    kubernetes_labels:
      '*': '*'
    kubernetes_resources:
      - kind: pod
        namespace: "*"
        name: "*"
    kubernetes_groups:
    - viewers
    kubernetes_users:
    # Replace USER with the Kubernetes user for your current context.
    - USER
  deny: {}
```

Apply your changes:

```code
$ tctl create -f kube-access.yaml
```

Assign the `kube-access` role to your Teleport user by running the following
commands, depending on whether you authenticate as a local Teleport user or via
the `github`, `saml`, or `oidc` authentication connectors:

<Tabs>
  <Tab title="Local User">
    Retrieve your local user's configuration resource:

    ```code
    $ tctl get users/$(tsh status -f json | jq -r '.active.username') > out.yaml
    ```

    Edit `out.yaml`, adding `kube-access` to the list of existing roles:

    ```diff
      roles:
       - access
       - auditor
       - editor
    +  - kube-access
    ```

    Apply your changes:

    ```code
    $ tctl create -f out.yaml
    ```
  </Tab>

  <Tab title="GitHub">
    Retrieve your `github`  configuration resource:

    ```code
    $ tctl get github/github --with-secrets > github.yaml
    ```

    Edit `github.yaml`, adding `kube-access` to the
    `teams_to_roles` section. The team you will map to this role will depend on how
    you have designed your organization's RBAC, but it should be the smallest team
    possible within your organization. This team must also include your user.

    Here is an example:

    ```diff
      teams_to_roles:
        - organization: octocats
          team: admins
          roles:
            - access
    +       - kube-access
    ```

    Apply your changes:

    ```code
    $ tctl create -f github.yaml
    ```

    <Warning>
      Note the `--with-secrets` flag in the `tctl get` command. This adds the value of
      `spec.signing_key_pair.private_key` to `github.yaml`. This is a sensitive value,
      so take precautions when creating this file and remove it after updating the resource.
    </Warning>
  </Tab>

  <Tab title="SAML">
    Retrieve your `saml`  configuration resource:

    ```code
    $ tctl get --with-secrets saml/mysaml > saml.yaml
    ```

    Edit `saml.yaml`, adding `kube-access` to the
    `attributes_to_roles` section. The attribute you will map to this role will
    depend on how you have designed your organization's RBAC, but it should be the
    smallest group possible within your organization. This group must also include
    your user.

    Here is an example:

    ```diff
      attributes_to_roles:
        - name: "groups"
          value: "my-group"
          roles:
            - access
    +       - kube-access
    ```

    Apply your changes:

    ```code
    $ tctl create -f saml.yaml
    ```

    <Warning>
      Note the `--with-secrets` flag in the `tctl get` command. This adds the value of
      `spec.signing_key_pair.private_key` to `saml.yaml`. This is a sensitive value,
      so take precautions when creating this file and remove it after updating the resource.
    </Warning>
  </Tab>

  <Tab title="OIDC">
    Retrieve your `oidc`  configuration resource:

    ```code
    $ tctl get oidc/myoidc --with-secrets > oidc.yaml
    ```

    Edit `oidc.yaml`, adding `kube-access` to the
    `claims_to_roles` section. The claim you will map to this role will depend on
    how you have designed your organization's RBAC, but it should be the smallest
    group possible within your organization. This group must also include your
    user.

    Here is an example:

    ```diff
      claims_to_roles:
        - name: "groups"
          value: "my-group"
          roles:
            - access
    +       - kube-access
    ```

    Apply your changes:

    ```code
    $ tctl create -f oidc.yaml
    ```

    <Warning>
      Note the `--with-secrets` flag in the `tctl get` command. This adds the value of
      `spec.signing_key_pair.private_key` to `oidc.yaml`. This is a sensitive value,
      so take precautions when creating this file and remove it after updating the resource.
    </Warning>
  </Tab>
</Tabs>

Log out of your Teleport cluster and log in again to assume the new role.

Now that Teleport RBAC is configured, you can authenticate to your Kubernetes
cluster via Teleport. To interact with your Kubernetes cluster, you will need to
configure authorization within Kubernetes.

### Kubernetes authorization

To configure authorization within your Kubernetes cluster, you need to create Kubernetes `RoleBinding`s or
`ClusterRoleBindings` that grant permissions to the subjects listed in `kubernetes_users` and
`kubernetes_groups`.

For example, you can grant some limited read-only permissions to the `viewers` group used in the `kube-access`
role defined above:

Create a file called `viewers-bind.yaml` with the following contents:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: viewers-crb
subjects:
- kind: Group
  # Bind the group "viewers", corresponding to the kubernetes_groups we assigned our "kube-access" role above
  name: viewers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  # "view" is a default ClusterRole that grants read-only access to resources
  # See: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles
  name: view
  apiGroup: rbac.authorization.k8s.io
```

Apply the `ClusterRoleBinding` with `kubectl`:

```code
$ kubectl apply -f viewers-bind.yaml
```

Log out of Teleport and log in again.

### Authorize your user to manage Kubernetes clusters

Teleport tracks Kubernetes clusters in your infrastructure via dynamic
`kube_cluster` resources. To manage access to Kubernetes clusters with Teleport,
your user will need permissions to manage these resources.

In the previous section, you authorized your user to access all Kubernetes
clusters registered in your Teleport cluster. Now that you can access these
clusters, create a role that enables you to manage them.

Create a role definition called `kube-manager.yaml` with the following content:

```yaml
kind: role
metadata:
  name: kube-manager
spec:
  allow:
    rules:
    - resources:
      - kube_cluster
      verbs:
      - list
      - create
      - read
      - update
      - delete
version: v5
```

Create the role:

```code
$ tctl create -f kube-manager.yaml
```

Assign the `kube-manager` role to your Teleport user by running the following
commands, depending on whether you authenticate as a local Teleport user or via
the `github`, `saml`, or `oidc` authentication connectors:

<Tabs>
  <Tab title="Local User">
    Retrieve your local user's configuration resource:

    ```code
    $ tctl get users/$(tsh status -f json | jq -r '.active.username') > out.yaml
    ```

    Edit `out.yaml`, adding `kube-manager` to the list of existing roles:

    ```diff
      roles:
       - access
       - auditor
       - editor
    +  - kube-manager
    ```

    Apply your changes:

    ```code
    $ tctl create -f out.yaml
    ```
  </Tab>

  <Tab title="GitHub">
    Retrieve your `github`  configuration resource:

    ```code
    $ tctl get github/github --with-secrets > github.yaml
    ```

    Edit `github.yaml`, adding `kube-manager` to the
    `teams_to_roles` section. The team you will map to this role will depend on how
    you have designed your organization's RBAC, but it should be the smallest team
    possible within your organization. This team must also include your user.

    Here is an example:

    ```diff
      teams_to_roles:
        - organization: octocats
          team: admins
          roles:
            - access
    +       - kube-manager
    ```

    Apply your changes:

    ```code
    $ tctl create -f github.yaml
    ```

    <Warning>
      Note the `--with-secrets` flag in the `tctl get` command. This adds the value of
      `spec.signing_key_pair.private_key` to `github.yaml`. This is a sensitive value,
      so take precautions when creating this file and remove it after updating the resource.
    </Warning>
  </Tab>

  <Tab title="SAML">
    Retrieve your `saml`  configuration resource:

    ```code
    $ tctl get --with-secrets saml/mysaml > saml.yaml
    ```

    Edit `saml.yaml`, adding `kube-manager` to the
    `attributes_to_roles` section. The attribute you will map to this role will
    depend on how you have designed your organization's RBAC, but it should be the
    smallest group possible within your organization. This group must also include
    your user.

    Here is an example:

    ```diff
      attributes_to_roles:
        - name: "groups"
          value: "my-group"
          roles:
            - access
    +       - kube-manager
    ```

    Apply your changes:

    ```code
    $ tctl create -f saml.yaml
    ```

    <Warning>
      Note the `--with-secrets` flag in the `tctl get` command. This adds the value of
      `spec.signing_key_pair.private_key` to `saml.yaml`. This is a sensitive value,
      so take precautions when creating this file and remove it after updating the resource.
    </Warning>
  </Tab>

  <Tab title="OIDC">
    Retrieve your `oidc`  configuration resource:

    ```code
    $ tctl get oidc/myoidc --with-secrets > oidc.yaml
    ```

    Edit `oidc.yaml`, adding `kube-manager` to the
    `claims_to_roles` section. The claim you will map to this role will depend on
    how you have designed your organization's RBAC, but it should be the smallest
    group possible within your organization. This group must also include your
    user.

    Here is an example:

    ```diff
      claims_to_roles:
        - name: "groups"
          value: "my-group"
          roles:
            - access
    +       - kube-manager
    ```

    Apply your changes:

    ```code
    $ tctl create -f oidc.yaml
    ```

    <Warning>
      Note the `--with-secrets` flag in the `tctl get` command. This adds the value of
      `spec.signing_key_pair.private_key` to `oidc.yaml`. This is a sensitive value,
      so take precautions when creating this file and remove it after updating the resource.
    </Warning>
  </Tab>
</Tabs>

Log out of your Teleport cluster and log in again to assume the new role.

## Step 3/3. Manage dynamic Kubernetes cluster resources

Now that your Teleport user has permissions to manage Kubernetes cluster
resources, we will show you how to create, list, update, and delete them.

### Create a kubeconfig

In this section, you will create a Kubernetes `Config` resource, or kubeconfig,
that your Teleport cluster will use to authenticate to your Kubernetes cluster.

When you signed into Teleport earlier in this guide, `tsh` may have changed your
Kubernetes context to one based on your Teleport cluster, so make sure you
update your Kubernetes context to match the cluster you would like to connect to
Teleport:

```code
$ kubectl config get-contexts
# Assign CONTEXT_NAME to your chosen context
$ kubectl config use-context CONTEXT_NAME
```

On your workstation, download Teleport's `get-kubeconfig.sh` script, which you
will use to generate the kubeconfig:

```code
$ curl -OL \
https://raw.githubusercontent.com/gravitational/teleport/v14.0.0-dev/examples/k8s-auth/get-kubeconfig.sh
```

The script creates a service account for the Teleport Kubernetes Service that
can get Kubernetes pods as well as impersonate users, groups, and other service
accounts. The Teleport Kubernetes Service uses this service account to manage
access to resources in your Kubernetes cluster. The script also ensures that
there is a Kubernetes `Secret` in your cluster to store service account
credentials.

`get-kubeconfig.sh` creates a namespace called `teleport` for the resources it
deploys, though you can choose a different name by assigning the
`TELEPORT_NAMESPACE` environment variable in the shell where you run the script.

After creating resources, `get-kubeconfig.sh` writes a new kubeconfig to a file
called `kubeconfig` in the directory where you run the script.

Run the `get-kubeconfig.sh` script:

```code
$ bash get-kubeconfig.sh
```

The script is successful if you see this message:

```text
Done!
```

Ignore the script's instructions to copy the generated kubeconfig file to the
Teleport Proxy Service. In the next section, we will show you how to use the
kubeconfig file when creating a dynamic `kube_cluster` resource.

### Create a Kubernetes cluster resource

Define a `kube_cluster` resource with the following content in a file called
`kube_cluster.yaml`:

```yaml
kind: kube_cluster
version: v3
metadata:
  name: mycluster
spec:
  kubeconfig: |
```

The `spec.kubeconfig` field in the snippet above begins a multi-line string.
Below, you will include the contents of the kubeconfig file as its value.

Since `spec.kubeconfig` must be a base64-encoded string, convert the kubeconfig
file to base64, then indent it and add it to the `kube_cluster.yaml` resource
definition using the following command:

```code
$ printf "    %s" $(cat kubeconfig | base64) >> kube_cluster.yaml
```

<Accordion title="Add labels to your kube_cluster">
  You can add labels to the `kube_cluster` resource, allowing you to manage access
  to specific clusters from your Teleport roles or Kubernetes Service instances.

  Labels can either be static or dynamic. Static labels are key/value pairs. This
  example defines the `env=prod` and `team=dev` labels:

  ```yaml
  kind: kube_cluster
  version: v3
  metadata:
    name: mycluster
    labels:
      env: prod
      team: dev
  spec:
    kubeconfig: KUBECONFIG
  ```

  You can also add dynamic labels, which define shell commands that a Kubernetes
  Service instance will execute in order to generate labels. To do so, edit the
  `spec.dynamic_labels` field of a `kube_cluster` resource.

  This example runs the `python3 get_region.py` command to fetch the region in
  which the Kubernetes Service is deployed and assign the result to the `region`
  key:

  ```yaml
  kind: kube_cluster
  version: v3
  metadata:
    name: mycluster
  spec:
    kubeconfig: KUBECONFIG
    dynamic_labels:
      region:
        period: "24h"
        command: ["python3", "get_region.py"]
  ```

  When defining a dynamic label, the key within the `spec.dynamic_labels` field
  behaves the same as keys within the `metadata.labels` field, indicating the key
  of the label.

  The Kubernetes Service obtains a value for that key by running the
  command given in `command` every `period`. `command` is an array of strings,
  where the first element indicates the command to execute and each subsequent
  element indicates an argument.

  `period` is a Go duration string, which includes a number and a unit of time.
  Supported units are `ns`, `us` (or `µs`), `ms`, `s`, `m`, and `h`. The example
  above configures the Kubernetes Service to run the command every day.
</Accordion>

To create the `kube_cluster` resource, run the following command:

```code
$ tctl create kube_cluster.yaml
kubernetes cluster "mycluster" has been created
```

### Access your new Kubernetes cluster

Instances of the Teleport Kubernetes Service watch for newly created or updated
`kube_cluster` resources. When you create the `kube_cluster` resource, any
Kubernetes Service instances you have configured to track that cluster's labels
will register that cluster and enable access to it via Teleport.

As a result, you should now see the cluster you registered above when you run
`tsh kube ls`:

```code
$ tsh kube ls
 Kube Cluster Name Labels                      Selected
 ----------------- --------------------------- --------
 mycluster         teleport.dev/origin=dynamic
```

The `teleport.dev/origin=dynamic` label indicates that the cluster was
registered dynamically.

You can also log in to the cluster you just registered:

```code
$ tsh kube login mycluster
Logged into kubernetes cluster "mycluster". Try 'kubectl version' to test the
connection.
```

### List Kubernetes cluster resources

You can list `kube_cluster` resources with the following command:

```code
$ tctl get kube_clusters
```

### Update a Kubernetes cluster resource

To update the `kube_cluster` resource you created earlier, execute the following
command to retrieve the resource as it exists on the Auth Service's backend:

```code
$ tctl get kube_clusters/mycluster > kube_cluster.yaml
```

Edit the `kube_cluster.yaml` file to add a label to your `kube_cluster`:

```diff
  kind: kube_cluster
  metadata:
    id: 9999999999999999999
    labels:
      teleport.dev/origin: dynamic
+     env: test
    name: mycluster
  spec:
    aws: {}
    azure: {}
    kubeconfig: KUBECONFIG
  version: v3
```

Update the resource:

```code
$ tctl create -f kube_cluster.yaml
kubernetes cluster "mycluster" has been updated
```

You should now see the updated labels:

```code
$ tsh kube ls
 Kube Cluster Name Labels                               Selected
 ----------------- ------------------------------------ --------
 mycluster         env=test teleport.dev/origin=dynamic *
```

<Warning>
  If the updated `kube_cluster` resource's labels no longer match the ones a Teleport
  Kubernetes Service instance is configured to watch, the instance will unregister
  and stop proxying the Kubernetes cluster.
</Warning>

### Delete Kubernetes cluster resources

To delete the `kube_cluster` resource you created earlier, run the following
command:

```code
$ tctl rm kube_clusters/mycluster
kubernetes cluster "mycluster" has been deleted
```

This will unregister the Kubernetes cluster from Teleport:

```code
$ tsh kube ls
Kube Cluster Name Labels Selected
----------------- ------ --------
```

## Next steps

In this guide, we showed you how to manage `kube_cluster` resources using
`tctl`. If you are interested in other ways you can manage access to Kubernetes
clusters via Teleport, check out the following guides:

- [Connect a Kubernetes Cluster to Teleport](/ver/14.x/kubernetes-access/getting-started): How to use
  the `teleport-kube-agent` Helm chart to register a Kubernetes cluster with
  Teleport.
- [Kubernetes Access from a Standalone Teleport
  Cluster](/ver/14.x/kubernetes-access/register-clusters/static-kubeconfig): How to use the Teleport Kubernetes
  Service's configuration file to register a Kubernetes cluster with Teleport.
