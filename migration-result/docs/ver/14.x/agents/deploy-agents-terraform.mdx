---
title: "Deploy Teleport Agents with Terraform"
description: "In this guide, we will show you how to deploy a pool of Teleport agents so you can apply dynamic resources to enroll your infrastructure with Teleport."
version: '14.x'
---

An agent is a Teleport instance configured to run one or more Teleport services
in order to proxy infrastructure resources. For a brief architectural overview
of how agents run in a Teleport cluster, read the [Introduction to Teleport
Agents](/docs/ver/14.x/agents/introduction).

This guide shows you how to deploy a pool of Teleport agents running on virtual
machines by declaring it as code using Terraform.

There are several methods you can use to join a Teleport agent to your cluster,
which we discuss in the [Joining Services to your
Cluster](/docs/ver/14.x/agents/join-services-to-your-cluster) guide. In this guide, we will use
the **join token** method, where the operator stores a secure token on the Auth
Service, and an agent presents the token in order to join a cluster.

No matter which join method you use, it will involve the following Terraform
resources:

- Compute instances to run Teleport services
- A join token for each compute instance in the agent pool

<img src="/assets/tf-agent-diagram-14b6901ef6.png" alt="A Teleport agent pool" width="1800" height="1028" />

## Prerequisites

<Tabs>
  <Tab title="Teleport Community Edition">
    - A running Teleport cluster. For details on how to set this up, see the
      [Getting Started](/docs/ver/14.x/index) guide.

    - The `tctl` admin tool and `tsh` client tool version >= 14.3.6.

      See [Installation](/docs/ver/14.x/installation) for details.

    To check version information, run the `tctl version` and `tsh version` commands.
    For example:

    ```code
    $ tctl version
    # Teleport v14.3.6 git:api/14.0.0-gd1e081e go1.21
      
    $ tsh version
    # Teleport v14.3.6 go1.21
    Proxy version: 14.3.6
    Proxy: teleport.example.com
    ```
  </Tab>

  <Tab title="Teleport Team">
    - A Teleport Team account. If you don't have an account, sign
      up to begin your [free trial](https://goteleport.com/signup/).

    - The Enterprise `tctl` admin tool and `tsh` client tool, version >= 14.3.6.

      You can download these tools from the [Cloud Downloads page](/docs/ver/14.x/choose-an-edition/teleport-cloud/downloads).

    To check version information, run the `tctl version` and `tsh version` commands.
    For example:

    ```code
    $ tctl version
    # Teleport Enterprise v14.3.6 git:api/14.0.0-gd1e081e go1.21
      
    $ tsh version
    # Teleport v14.3.6 go1.21
    Proxy version: 14.3.6
    Proxy: teleport.example.com
    ```
  </Tab>

  <Tab title="Teleport Enterprise">
    - A running Teleport Enterprise cluster. For details on how to set this up, see the Enterprise
      [Getting Started](/docs/ver/14.x/choose-an-edition/teleport-enterprise/introduction) guide.

    - The Enterprise `tctl` admin tool and `tsh` client tool version >=
      14.3.6.

      You can download these tools by visiting your [Teleport
      account workspace](https://teleport.sh).

    To check version information, run the `tctl version` and `tsh version` commands.
    For example:

    ```code
    $ tctl version
    # Teleport Enterprise v14.3.6 git:api/14.0.0-gd1e081e go1.21
      
    $ tsh version
    # Teleport v14.3.6 go1.21
    Proxy version: 14.3.6
    Proxy: teleport.example.com
    ```
  </Tab>

  <Tab title="Teleport Enterprise Cloud">
    - A Teleport Enterprise Cloud account. If you don't have an account,
      sign up to begin a [free trial](https://goteleport.com/signup/)  of Teleport Team
      and upgrade to Teleport Enterprise Cloud.

    - The Enterprise `tctl` admin tool and `tsh` client tool version >= 14.3.6.

      You can download these tools from the [Cloud Downloads page](/docs/ver/14.x/choose-an-edition/teleport-cloud/downloads).

    To check version information, run the `tctl version` and `tsh version` commands.
    For example:

    ```code
    $ tctl version
    # Teleport Enterprise v14.3.6 git:api/14.0.0-gd1e081e go1.21
      
    $ tsh version
    # Teleport v14.3.6 go1.21
    Proxy version: 14.3.6
    Proxy: teleport.example.com
    ```
  </Tab>
</Tabs>

<Tip>
  We recommend following this guide on a fresh Teleport demo cluster so you can
  see how an agent pool works. After you are familiar with the setup, apply the
  lessons from this guide to protect your infrastructure. You can get started with
  a demo cluster using:

  - A demo deployment on a [Linux server](/docs/ver/14.x/index)
  - A [Teleport Team trial](https://goteleport.com/signup)
</Tip>

- An AWS account with permissions to create virtual machine instances.

- Cloud infrastructure that enables virtual machine instances to connect to the
  Teleport Proxy Service, such as an AWS subnet with a public NAT gateway or NAT
  instance.

  For demo purposes, you can also configure the VM instances you deploy to
  have public IP addresses.

- Terraform v1.0.0 or higher.

- An identity file for the Teleport Terraform provider. Make sure you are
  familiar with [how to set up the Teleport Terraform
  provider](/docs/ver/14.x/management/dynamic-resources/terraform-provider) before
  following this guide.

- To check that you can connect to your Teleport cluster, sign in with `tsh login`, then
  verify that you can run `tctl` commands using your current credentials.
  `tctl` is supported on macOS and Linux machines.

  For example:
  ```code
  $ tsh login --proxy=teleport.example.com --user=email@example.com
  $ tctl status
  # Cluster  teleport.example.com
  # Version  14.3.6
  # CA pin   sha256:abdc1245efgh5678abdc1245efgh5678abdc1245efgh5678abdc1245efgh5678
  ```
  If you can connect to the cluster and run the `tctl status` command, you can use your
  current credentials to run subsequent `tctl` commands from your workstation.
  If you host your own Teleport cluster, you can also run `tctl` commands on the computer that
  hosts the Teleport Auth Service for full permissions.

## Step 1/4. Fetch the example Terraform configuration

1. Fetch the Teleport code repository and copy the example Terraform
   configuration for this project into your current working directory:

   ```code
   $ git clone --depth=1 https://github.com/gravitational/teleport
   $ cp -R teleport/examples/agent-pool-terraform .
   $ rm -rf teleport
   ```

2. Move the identity file for the Teleport Terraform provider into your project
   directory so the Terraform provider an access it. Name the file
   `terraform-identity`.

   <Warning>
     If you don't have an identity file available, make sure you have followed the
     [prerequisites for this guide](#prerequisites).
   </Warning>

## Step 2/4. Review your Terraform configuration

In this section, we explain the resources configured in the Terraform module
that you copied earlier. We then show you how to assign input variables so you
can deploy an agent pool in your infrastructure.

### Instances and tokens

In this minimal example, we deploy one virtual machine instance for each
Teleport agent. Each agent joins the cluster using a token. We create each token
using the `teleport_provision_token` Terraform resource, specifying the token's
value with a `random_string` resource:

```hcl
resource "random_string" "token" {
  count  = var.agent_count
  length = 32
}

resource "teleport_provision_token" "agent" {
  count = var.agent_count
  spec = {
    roles = [
      // Uncomment the roles that correspond to the Teleport services you plan
      // to run on your agent nodes. We recommend running the SSH Service at a
      // minimum to enable secure access to the nodes.
      // "App",
      // "Db",
      // "Discovery",
      // "Kube",
      "Node",
    ]
    name = random_string.token[count.index].result
  }
  metadata = {
    expires = timeadd(timestamp(), "1h")
  }
}

```

When we apply the `teleport_provision_token` resources, the Teleport Terraform
provider creates them on the Teleport Auth Service backend.

The Teleport Auth Service associates the join token with one or more roles,
identifying the Teleport service that is allowed to use the token.  Modify
`token.tf` to uncomment the token roles that correspond to the Teleport services
you plan to run on your agent nodes:

- Teleport Application Service (`App`)
- Teleport Discovery Service (`Discovery`)
- Teleport Database Service (`Db`)
- Teleport Kubernetes Service (`Kube`)
- Teleport SSH Service (`Node`)

Each virtual machine instance presents the token in order to establish trust
with the cluster.

`agent-pool.tf` declares a data source for an Amazon Linux 2023 machine image
and uses it to launch EC2 instances that run Teleport agents with the
`teleport_provision_token` resource:

```hcl
data "aws_ami" "amazon_linux_2023" {
  most_recent = true

  filter {
    name   = "description"
    values = ["Amazon Linux 2023 AMI*"]
  }

  filter {
    name   = "architecture"
    values = ["x86_64"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }

  filter {
    name   = "owner-alias"
    values = ["amazon"]
  }
}

resource "aws_instance" "teleport_agent" {
  count         = var.agent_count
  ami           = data.aws_ami.amazon_linux_2023.id
  instance_type = "t3.small"
  subnet_id     = var.subnet_id
  user_data = templatefile("./userdata", {
    token                 = teleport_provision_token.agent[count.index].metadata.name
    proxy_service_address = var.proxy_service_address
    teleport_edition      = var.teleport_edition
    teleport_version      = var.teleport_version
  })

  // The following two blocks adhere to security best practices.

  associate_public_ip_address = false
  monitoring                  = true

  metadata_options {
    http_endpoint = "enabled"
    http_tokens   = "required"
  }

  root_block_device {
    encrypted = true
  }
}

```

If you are deploying your instance in a demo environment and do not have a NAT
gateway, NAT instance, or other method for connecting your instances to the
Teleport Proxy Service, modify the `aws_instance` resource to associate a public
IP address with it:

```hcl
associate_public_ip_address = true
```

### Startup script

Each EC2 instance runs a script on startup, which we configured above using the
`user_data` field within the `aws_instance.teleport_agent` resource
(`examples/agent-pool-terraform/userdata`):

```text
#!/bin/bash

curl https://goteleport.com/static/install.sh | bash -s ${teleport_version} ${teleport_edition}

# Edit the configuration below to enable any services you plan to run on your
# Teleport agent nodes. We recommend running the SSH Service at a minimum to
# enable secure access to the nodes.
echo ${token} > /var/lib/teleport/token
cat<<EOF >/etc/teleport.yaml
version: v3
teleport:
  auth_token: /var/lib/teleport/token
  proxy_server: ${proxy_service_address}
app_service:
  enabled: false
  resources:
  - labels:
      "*": "*"
auth_service:
  enabled: false
db_service:
  enabled: false
  resources:
  - labels:
      "*": "*"
discovery_service:
  enabled: false
kubernetes_service:
  enabled: false
  resources:
  - labels:
      "*": "*"
proxy_service:
  enabled: false
ssh_service:
  enabled: true
  labels:
    role: agent-pool
EOF

systemctl restart teleport;

```

This script installs Teleport Community Edition on the host, then writes a
configuration file to the default location, `/etc/teleport.yaml`. The
configuration file enables each Teleport service we associated with our token.

The configuration also adds the `role: agent-pool` label to the Teleport SSH
Service on each instance. This makes it easier to access hosts in the agent pool
later.

Finally, the script restarts Teleport on the host to apply the new
configuration.

In the example above, only the Teleport SSH Service is enabled. Edit the script
to enable each Teleport service you plan to run on your agent nodes by making
the following change for each service you want to enable:

```diff
-  enabled: false
+  enabled: true
```

Make sure you have configured the `teleport_provision_token` resource to include
a role that corresponds to each service you plan to run. For example, if you
enable the Teleport Kubernetes Service, the token must have the `Kube` role.

## Step 3/4. Assign input variables

In this step, you will configure the Terraform module for your environment.

1. In your `agent-pool-terraform` project directory, create a file called
   `main.auto.tfvars` with the following content:

   ```hcl
   agent_count=2
   proxy_service_address="mytenant.teleport.sh"
   aws_region=""
   teleport_version="14.3.6"
   teleport_edition="oss"
   subnet_id=""
   ```

2. `agent_count` is set to `2` for high availability. As you scale your Teleport
   usage, you can increase this count to ease the load on each agent. You can
   consider adding your agents to an Auto Scaling group as well.

3. Assign `proxy_service_address` to the host and HTTPS port of your Teleport
   Proxy Service, e.g., `mytenant.teleport.sh:443`.

   <Tip>
     Make sure to include the port.
   </Tip>

4. Assign `aws_region` to the AWS region where you plan to deploy Teleport
   agents, such as `us-east-1`.

5. If needed, change the value of `teleport_version` to the version of Teleport
   you want to run on your agents. It must be either the same major version as
   your Teleport cluster or one major version behind.

6. Make sure `teleport_edition` matches your Teleport edition. Assign this to
   `oss`, `cloud`, `enterprise`, or `team`. The default is `oss`.

7. For `subnet_id`, include the ID of the AWS subnet (beginning with `subnet-`)
   where you will deploy Teleport agents.

8. Make sure you are using the latest supported version of the Teleport
   Terraform provider. In `providers-aws.tf`, we configure the Terraform
   provider version with a placeholder value, `TELEPORT_VERSION`.

   Replace the placeholder value with the latest version:

   ```code
   $ sed -i "" "s/TELEPORT_VERSION/14.3.3/" provider.tf
   ```

## Step 4/4. Verify the deployment

Make sure your cloud provider credentials are available to Terraform using the
standard approach for your organization.

Apply the Terraform configuration:

```code
$ terraform init
$ terraform apply
```

Once the `apply` command completes, run the following command to verify that the
two agents have deployed successfully:

```code
$ tsh ls role=agent-pool
Node Name                  Address    Labels
-------------------------- ---------- ---------------
ip-10-1-1-187.ec2.internal ⟵ Tunnel   role=agent-pool
ip-10-1-1-24.ec2.internal  ⟵ Tunnel   role=agent-pool
```

## Next step: Enroll infrastructure resources

There are two ways to configure your agent pool to protect infrastructure
resources with Teleport, which we describe below.

### Define dynamic resources in Terraform

You can declare Terraform resources that enroll your infrastructure with
Teleport. The Teleport Terraform provider currently supports the following:

| Infrastructure Resource | Terraform Resource  |
| ----------------------- | ------------------- |
| Application             | `teleport_app`      |
| Database                | `teleport_database` |

To declare a dynamic resource with Terraform, add a configuration block similar
to the ones below to a `*.tf` file in your `agent-pool-terraform` project
directory.

The Teleport Terraform provider creates these on the Auth Service backend, and
the relevant Teleport services query them in order to proxy user traffic. For a
full list of supported resources and fields, see the [Terraform provider
reference](/docs/ver/14.x/reference/terraform-provider).

<Tabs>
  <Tab title="Application">
    ```hcl
    resource "teleport_app" "example" {
      metadata = {
        name        = "example"
        description = "Test app"
        labels = {
          // Teleport adds this label by default, so add it here to
          // ensure a consistent state.
          "teleport.dev/origin" = "dynamic"
        }
      }

      spec = {
        uri = "localhost:3000"
      }
    }
    ```
  </Tab>

  <Tab title="Database">
    ```hcl
    resource "teleport_database" "example" {
      metadata = {
        name        = "example"
        description = "Test database"
        labels = {
          // Teleport adds this label by default, so add it here to
          // ensure a consistent state.
          "teleport.dev/origin" = "dynamic"
        }
      }

      spec = {
        protocol = "postgres"
        uri      = "localhost"
      }
    }
    ```
  </Tab>
</Tabs>

### Configure Teleport services in the agent pool

Each Teleport service reads its local configuration file (`/etc/teleport.yaml`
by default) to determine which infrastructure resources to proxy. You can edit
this configuration file to enroll resources with Teleport.

In the setup we explored in this guide, you can edit the user data script for
each instance to add configuration settings to, for example, the
`database_service` or `kubernetes_service` sections.

To see how to configure each service, read its section of the documentation:

- [SSH Service](/docs/ver/14.x/server-access/introduction)
- [Database Service](/docs/ver/14.x/database-access/introduction)
- [Kubernetes Service](/docs/ver/14.x/kubernetes-access/introduction)
- [Windows Desktop Service](/docs/ver/14.x/desktop-access/introduction)
- [Application Service](/docs/ver/14.x/application-access/introduction)
